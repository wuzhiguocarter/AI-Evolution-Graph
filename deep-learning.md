

- 1943年，麦卡洛克和皮兹提出了神经网络和数学模型，称为MCP模型，是人工神经网络的开端。
- 1958年，罗森布拉特提出了两层神经元组成的神经网络，称之为感知器，是机器学习分类的第一个方法。
- 1969年，明斯基证明了感知器只能处理线性分类问题，无法解决XOR等非线性问题，导致神经网络的研究陷入停滞。
- 1986年，Hinton发明了适用于多层感知器的反向传播算法，并采用Sigmoid进行非线性映射，有效解决了非线性分类和学习的问题，引起了神经网络的第二次热潮。
- 1991年，BP算法被指出存在梯度消失问题，导致深层网络难以训练。同时，支持向量机等浅层机器学习模型被提出，竞争了神经网络的地位。
- 2006年，Hinton提出了无监督预训练对权值进行初始化+有监督训练微调的方法，有效解决了梯度消失问题，并在语音识别和图像识别等领域取得了突破性的效果，开启了深度学习在学术界和工业界的浪潮。
- 2011年，ReLU激活函数被提出，能够有效地抑制梯度消失问题。微软和谷歌等公司将深度学习应用在语音识别上，降低了错误率。
- 2012年，Hinton课题组参加ImageNet图像识别比赛，通过构建的CNN网络AlexNet一举夺得冠军，并引起了CNN在视觉任务上的广泛关注和应用。
- 2016年，谷歌旗下 DeepMind 公司开发的AlphaGo利用深度学习与围棋世界冠军李世石进行围棋人机大战，并以4比1的总比分获胜，展示了深度学习在复杂策略游戏上的强大能力。
- 2021年，深度学习方面取得了多方面的关键性进展，如Transformer架构在各个领域的广泛应用和创新，万亿级参数的大模型的发布和应用，多模态AI的起飞和突破等。
	- 机器学习Transformer架构发展编年史的概述如下：
	- 2017年，Google Brain团队提出了Transformer模型，它是一种基于自注意力机制的深度学习模型，主要用于自然语言处理（NLP）和计算机视觉（CV）领域。¹
	- Transformer模型采用了编码器-解码器结构，但不依赖于循环和卷积来生成输出。它可以处理序列输入数据，如自然语言，并应用于翻译和文本摘要等任务。¹³
	- Transformer模型相比循环神经网络（RNN）模型，更适合并行化，可以在更大的数据集上进行训练。这促进了预训练系统的发展，如BERT（Bidirectional Encoder Representations from Transformers）和GPT（Generative Pre-trained Transformer），它们使用了大规模的语言数据集，如维基百科语料库和Common Crawl，并可以针对特定任务进行微调。
	  collapsed:: true
		- BERT
			- BERT（Bidirectional Encoder Representations from Transformers）是一种预训练语言模型，由Google开发。它是一种基于Transformer架构的深度学习模型，可以对大量的自然语言文本进行训练，从而获得通用的语言表示。
			- BERT的主要特点在于它是一个双向模型，可以同时考虑一个句子的左右两个方向，从而更好地理解句子的语义。此外，BERT还具有以下特点：
				- 1.  预训练模式：BERT使用无监督的方式进行预训练，即使用大量的文本数据进行训练，而不需要标注的数据。这使得BERT可以学习到通用的语言表示，从而可以用于各种自然语言处理任务。
				- 2.  Fine-tuning模式：在完成预训练后，BERT可以通过微调（fine-tuning）来适应特定的自然语言处理任务。例如，可以将BERT用于文本分类、命名实体识别、句子相似度等任务。
				- 3.  多语言支持：BERT可以用于多种语言，包括英语、中文、西班牙语等。这使得BERT成为了跨语言自然语言处理的有力工具。
			- BERT在多个自然语言处理任务中表现优秀，如GLUE任务、SQuAD任务等。它已经成为了自然语言处理领域的重要技术，被广泛应用于各种实际应用中。
		- GPT
			- GPT（Generative Pre-trained Transformer）是一种基于Transformer架构的预训练语言模型，由OpenAI开发。它是一种单向模型，只考虑一个句子的左侧上下文，从而生成下一个词的概率分布。
			- GPT的主要特点在于它可以通过无监督的方式进行预训练，即使用大量的文本数据进行训练，而不需要标注的数据。在预训练过程中，GPT主要使用了两种技术：自回归模型和掩码语言模型。
				- 自回归模型是指模型在生成一个词时，会考虑前面所有的词，从而生成下一个词。这种模型可以用于生成文本、机器翻译等任务。
				- 掩码语言模型是指模型在训练时，会随机将一些词替换为特殊的掩码符号，然后让模型预测这些掩码符号代表的词。这种模型可以用于填充空白、命名实体识别等任务。
			- GPT在多个自然语言处理任务中表现优秀，如文本分类、命名实体识别、机器翻译等。它已经成为了自然语言处理领域的重要技术，被广泛应用于各种实际应用中
	- 2018年， OpenAI 发布了GPT-2模型，它是一种基于Transformer的大规模语言模型，可以生成连贯和多样的文本。
	- 2019年，Google发布了BERT模型，它是一种基于Transformer的双向语言模型，可以利用上下文信息来理解自然语言，并在多个NLP任务上取得了最先进的性能。
	- 2020年，OpenAI发布了GPT-3模型，它是一种基于Transformer的巨型语言模型，拥有1750亿个参数，并可以根据给定的输入生成各种类型的文本。
	- 2021年，Google发布了 Switch Transformer 模型，它是一种基于Transformer的混合并行模型，可以在多个设备上分布式训练，并实现了超过100万亿个参数的规模。
