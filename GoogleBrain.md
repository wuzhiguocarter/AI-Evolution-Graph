- [2011年，谷歌大脑的雏形起源于一项斯坦福大学与谷歌公司的联合研究项目，由**杰夫·迪恩**、**格雷格·科拉多**和**吴恩达**等人组成](https://zh.wikipedia.org/wiki/%E8%B0%B7%E6%AD%8C%E5%A4%A7%E8%84%91)[1](https://zh.wikipedia.org/wiki/%E8%B0%B7%E6%AD%8C%E5%A4%A7%E8%84%91)[2](https://research.google/teams/brain/)。
- [2012年6月，谷歌大脑搭建了一个由16000台电脑组成的人工神经网络，能够通过观看YouTube视频自行学习并识别出“猫”的概念](https://zhuanlan.zhihu.com/p/24646490)[3](https://zhuanlan.zhihu.com/p/24646490)。
- 2013年3月，谷歌聘用了深度学习领域的专家杰弗里·辛顿，并收购了他所领导的DNNResearch公司。
- 2014年1月，谷歌收购了英国的人工智能公司DeepMind，并将其与谷歌大脑合并。
- 2015年11月，谷歌大脑发布了TensorFlow，一个开源的深度学习框架，用于构建和训练各种机器学习模型。
- 2016年9月，谷歌大脑发布了Google Neural Machine Translation（GNMT），一个基于神经网络的机器翻译系统，能够提高谷歌翻译的质量和准确性。
- 2017年5月，谷歌大脑发布了AutoML，一个自动化机器学习平台，能够根据用户的数据和目标自动生成和优化机器学习模型。
- 2018年10月，谷歌大脑发布了BERT，一个基于Transformer的自然语言理解模型，能够在多种自然语言处理任务上取得最先进的效果。
- 2019年5月，谷歌大脑发布了Translatotron，一个端到端的语音翻译系统，能够保留源语音的音色和节奏。
- 2020年6月，谷歌大脑发布了BigBird，一个扩展了BERT的自然语言理解模型，能够处理更长的文本序列。
